{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import accelerate\n",
    "\n",
    "# Grounding DINO\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# diffusers\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "model_config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "model_checkpoint_path = \"groundingdino_swint_ogc.pth\"\n",
    "\n",
    "\n",
    "groundingdino_model = load_model(model_config_path, model_checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = '/root/Grounded-Segment-Anything/sam_vit_h_4b8939.pth'\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_image_path = \"product0.png\"\n",
    "\n",
    "image_source, image = load_image(local_image_path)\n",
    "Image.fromarray(image_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect object using grounding DINO\n",
    "def detect(image, text_prompt, model, box_threshold = 0.3, text_threshold = 0.25):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
    "    return annotated_frame, boxes\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords = None,\n",
    "        point_labels = None,\n",
    "        boxes = transformed_boxes,\n",
    "        multimask_output = False,\n",
    "        )\n",
    "    return masks.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/Grounded-Segment-Anything/test.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m annotated_frame, detected_boxes \u001b[39m=\u001b[39m detect(image, text_prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mphones\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49mgroundingdino_model)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m segmented_frame_masks \u001b[39m=\u001b[39m segment(image_source, sam_predictor, boxes\u001b[39m=\u001b[39mdetected_boxes)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m mask0 \u001b[39m=\u001b[39m segmented_frame_masks[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m/root/Grounded-Segment-Anything/test.ipynb Cell 8\u001b[0m line \u001b[0;36mdetect\u001b[0;34m(image, text_prompt, model, box_threshold, text_threshold)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(image, text_prompt, model, box_threshold \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m, text_threshold \u001b[39m=\u001b[39m \u001b[39m0.25\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     boxes, logits, phrases \u001b[39m=\u001b[39m predict(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         image\u001b[39m=\u001b[39;49mimage,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         caption\u001b[39m=\u001b[39;49mtext_prompt,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         box_threshold\u001b[39m=\u001b[39;49mbox_threshold,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         text_threshold\u001b[39m=\u001b[39;49mtext_threshold\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     annotated_frame \u001b[39m=\u001b[39m annotate(image_source\u001b[39m=\u001b[39mimage_source, boxes\u001b[39m=\u001b[39mboxes, logits\u001b[39m=\u001b[39mlogits, phrases\u001b[39m=\u001b[39mphrases)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-45.autodl.pro/root/Grounded-Segment-Anything/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     annotated_frame \u001b[39m=\u001b[39m annotated_frame[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m# BGR to RGB\u001b[39;00m\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/util/inference.py:67\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, image, caption, box_threshold, text_threshold, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     66\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 67\u001b[0m     outputs \u001b[39m=\u001b[39m model(image[\u001b[39mNone\u001b[39;49;00m], captions\u001b[39m=\u001b[39;49m[caption])\n\u001b[1;32m     69\u001b[0m prediction_logits \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mpred_logits\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msigmoid()[\u001b[39m0\u001b[39m]  \u001b[39m# prediction_logits.shape = (nq, 256)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m prediction_boxes \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mpred_boxes\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcpu()[\u001b[39m0\u001b[39m]  \u001b[39m# prediction_boxes.shape = (nq, 4)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py:313\u001b[0m, in \u001b[0;36mGroundingDINO.forward\u001b[0;34m(self, samples, targets, **kw)\u001b[0m\n\u001b[1;32m    310\u001b[0m         poss\u001b[39m.\u001b[39mappend(pos_l)\n\u001b[1;32m    312\u001b[0m input_query_bbox \u001b[39m=\u001b[39m input_query_label \u001b[39m=\u001b[39m attn_mask \u001b[39m=\u001b[39m dn_meta \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m hs, reference, hs_enc, ref_enc, init_box_proposal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    314\u001b[0m     srcs, masks, input_query_bbox, poss, input_query_label, attn_mask, text_dict\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    317\u001b[0m \u001b[39m# deformable-detr-like anchor update\u001b[39;00m\n\u001b[1;32m    318\u001b[0m outputs_coord_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py:258\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, srcs, masks, refpoint_embed, pos_embeds, tgt, attn_mask, text_dict)\u001b[0m\n\u001b[1;32m    253\u001b[0m enc_topk_proposals \u001b[39m=\u001b[39m enc_refpoint_embed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m#########################################################\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# Begin Encoder\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m#########################################################\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m memory, memory_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    259\u001b[0m     src_flatten,\n\u001b[1;32m    260\u001b[0m     pos\u001b[39m=\u001b[39;49mlvl_pos_embed_flatten,\n\u001b[1;32m    261\u001b[0m     level_start_index\u001b[39m=\u001b[39;49mlevel_start_index,\n\u001b[1;32m    262\u001b[0m     spatial_shapes\u001b[39m=\u001b[39;49mspatial_shapes,\n\u001b[1;32m    263\u001b[0m     valid_ratios\u001b[39m=\u001b[39;49mvalid_ratios,\n\u001b[1;32m    264\u001b[0m     key_padding_mask\u001b[39m=\u001b[39;49mmask_flatten,\n\u001b[1;32m    265\u001b[0m     memory_text\u001b[39m=\u001b[39;49mtext_dict[\u001b[39m\"\u001b[39;49m\u001b[39mencoded_text\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    266\u001b[0m     text_attention_mask\u001b[39m=\u001b[39;49m\u001b[39m~\u001b[39;49mtext_dict[\u001b[39m\"\u001b[39;49m\u001b[39mtext_token_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    267\u001b[0m     \u001b[39m# we ~ the mask . False means use the token; True means pad the token\u001b[39;49;00m\n\u001b[1;32m    268\u001b[0m     position_ids\u001b[39m=\u001b[39;49mtext_dict[\u001b[39m\"\u001b[39;49m\u001b[39mposition_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    269\u001b[0m     text_self_attention_masks\u001b[39m=\u001b[39;49mtext_dict[\u001b[39m\"\u001b[39;49m\u001b[39mtext_self_attention_masks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    271\u001b[0m \u001b[39m#########################################################\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m# End Encoder\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39m# - memory: bs, \\sum{hw}, c\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39m# - enc_intermediate_refpoints: None or (nenc+1, bs, nq, c) or (nenc, bs, nq, c)\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m#########################################################\u001b[39;00m\n\u001b[1;32m    279\u001b[0m text_dict[\u001b[39m\"\u001b[39m\u001b[39mencoded_text\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m memory_text\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py:576\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, pos, spatial_shapes, level_start_index, valid_ratios, key_padding_mask, memory_text, text_attention_mask, pos_text, text_self_attention_masks, position_ids)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39m# main process\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_transformer_ckpt:\n\u001b[0;32m--> 576\u001b[0m     output \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[1;32m    577\u001b[0m         layer,\n\u001b[1;32m    578\u001b[0m         output,\n\u001b[1;32m    579\u001b[0m         pos,\n\u001b[1;32m    580\u001b[0m         reference_points,\n\u001b[1;32m    581\u001b[0m         spatial_shapes,\n\u001b[1;32m    582\u001b[0m         level_start_index,\n\u001b[1;32m    583\u001b[0m         key_padding_mask,\n\u001b[1;32m    584\u001b[0m     )\n\u001b[1;32m    585\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     output \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    587\u001b[0m         src\u001b[39m=\u001b[39moutput,\n\u001b[1;32m    588\u001b[0m         pos\u001b[39m=\u001b[39mpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m         key_padding_mask\u001b[39m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    593\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:235\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    238\u001b[0m         function,\n\u001b[1;32m    239\u001b[0m         preserve,\n\u001b[1;32m    240\u001b[0m         \u001b[39m*\u001b[39margs\n\u001b[1;32m    241\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:96\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     93\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 96\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py:785\u001b[0m, in \u001b[0;36mDeformableTransformerEncoderLayer.forward\u001b[0;34m(self, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    781\u001b[0m     \u001b[39mself\u001b[39m, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    782\u001b[0m ):\n\u001b[1;32m    783\u001b[0m     \u001b[39m# self attention\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39m# import ipdb; ipdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 785\u001b[0m     src2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    786\u001b[0m         query\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwith_pos_embed(src, pos),\n\u001b[1;32m    787\u001b[0m         reference_points\u001b[39m=\u001b[39;49mreference_points,\n\u001b[1;32m    788\u001b[0m         value\u001b[39m=\u001b[39;49msrc,\n\u001b[1;32m    789\u001b[0m         spatial_shapes\u001b[39m=\u001b[39;49mspatial_shapes,\n\u001b[1;32m    790\u001b[0m         level_start_index\u001b[39m=\u001b[39;49mlevel_start_index,\n\u001b[1;32m    791\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    792\u001b[0m     )\n\u001b[1;32m    793\u001b[0m     src \u001b[39m=\u001b[39m src \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(src2)\n\u001b[1;32m    794\u001b[0m     src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(src)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/ms_deform_attn.py:338\u001b[0m, in \u001b[0;36mMultiScaleDeformableAttention.forward\u001b[0;34m(self, query, key, value, query_pos, key_padding_mask, reference_points, spatial_shapes, level_start_index, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     sampling_locations \u001b[39m=\u001b[39m sampling_locations\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    336\u001b[0m     attention_weights \u001b[39m=\u001b[39m attention_weights\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m--> 338\u001b[0m output \u001b[39m=\u001b[39m MultiScaleDeformableAttnFunction\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    339\u001b[0m     value,\n\u001b[1;32m    340\u001b[0m     spatial_shapes,\n\u001b[1;32m    341\u001b[0m     level_start_index,\n\u001b[1;32m    342\u001b[0m     sampling_locations,\n\u001b[1;32m    343\u001b[0m     attention_weights,\n\u001b[1;32m    344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim2col_step,\n\u001b[1;32m    345\u001b[0m )\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m halffloat:\n\u001b[1;32m    348\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mhalf()\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/ms_deform_attn.py:53\u001b[0m, in \u001b[0;36mMultiScaleDeformableAttnFunction.forward\u001b[0;34m(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     44\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     im2col_step,\n\u001b[1;32m     51\u001b[0m ):\n\u001b[1;32m     52\u001b[0m     ctx\u001b[39m.\u001b[39mim2col_step \u001b[39m=\u001b[39m im2col_step\n\u001b[0;32m---> 53\u001b[0m     output \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39mms_deform_attn_forward(\n\u001b[1;32m     54\u001b[0m         value,\n\u001b[1;32m     55\u001b[0m         value_spatial_shapes,\n\u001b[1;32m     56\u001b[0m         value_level_start_index,\n\u001b[1;32m     57\u001b[0m         sampling_locations,\n\u001b[1;32m     58\u001b[0m         attention_weights,\n\u001b[1;32m     59\u001b[0m         ctx\u001b[39m.\u001b[39mim2col_step,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(\n\u001b[1;32m     62\u001b[0m         value,\n\u001b[1;32m     63\u001b[0m         value_spatial_shapes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         attention_weights,\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "annotated_frame, detected_boxes = detect(image, text_prompt=\"phones\", model=groundingdino_model)\n",
    "segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
    "mask0 = segmented_frame_masks[0][0].cpu().numpy()\n",
    "inverted_mask = ((1 - mask0) * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
